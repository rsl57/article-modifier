import sys
import os
import re
import input_validation
import nltk
import random
import requests
from bs4 import BeautifulSoup
from newsapi import NewsApiClient

# Reads the file and splits the result into its respective sentences
def get_sentences(filename:str):
    # Read the file
    infile = open(filename, "r")
    file_contents = infile.read()

    # Split the file into sentences
    nltk.download('punkt')  
    return nltk.sent_tokenize(file_contents)

def replace_random_sentence(sentences):
    # Select a random sentence to replace
    sentence_to_replace = random.choice(sentences)

    # Generate a random sentence to replace it
    new_sentence = "This is a new random sentence."

    # Replace the selected sentence with the new one
    sentences[sentences.index(sentence_to_replace)] = new_sentence

    return sentences

# returns the HTML content of the article
def get_article_content(article):
    response = requests.get(article['url'])
    # Fetch the full content of the article by parsing the HTML of the article page
    if response.status_code == 200: #success
        return response.content
    else:
        print(f"Error: Failed to retrieve article content. Response status code: {response.status_code}")

# returns a list of information about an article generated by newsapi
def get_article(api):
    try:
        # Get a list of top headlines
        top_headlines = api.get_top_headlines()
        # Choose a random article from the list
        article = random.choice(top_headlines['articles'])
        return article
    except requests.exceptions.RequestException as e:
        print(f"Error: Failed to retrieve article content. Exception: {e}")
        print("trying again...")
        return get_article(api)

def parse_html(soup):
    destroy_tags = [
        'header',
        'footer',
        'img',
        'iframe',
        'button',
        'aside',
        'figcaption',
        'ul',
        'ol'
    ]
    for tag in destroy_tags:
        tagList = soup.find_all(tag)
        if len(tagList) > 0:
            for t in tagList:
                t.extract()

    return soup.find('article').get_text(separator=" ")


if __name__ == "__main__":
    # Check if user put in their own article
    if len(sys.argv) >= 2:
        input_validation.check_args(sys.argv)
        input_validation.check_file_valid(sys.argv[1])
        sentences = get_sentences(sys.argv[1])
        new_sentences = replace_random_sentence(sentences)
            
        # Create a new file name with '_modified' added to the original filename
        file_name, file_extension = os.path.splitext(sys.argv[1])
        new_file_name = file_name + '_modified' + file_extension

        # write new text to file
        with open(new_file_name, 'w') as f:
            for sentence in new_sentences:
                f.write(sentence + "\n")

        print(f'New sentences written to file {new_file_name}')
    # if user didnt put in their own file, grab one using news api
    else:
        # Initialize the NewsAPI client with your API key
        newsapi = NewsApiClient(api_key='88d3ee5544a34e0387ae1c54b74ec5f0')

        soup = BeautifulSoup()
        while soup.find('article') == None:
            article = get_article(newsapi)
            article_content = get_article_content(article)
            soup = BeautifulSoup(article_content, 'html.parser')
        
        article_text = parse_html(soup)
        # format article name
        article_title = article['title']
        article_title = article_title.replace("/", "")
        article_title = article_title.replace(" ", "_")
        article_title = re.sub('[^0-9a-zA-Z]+', '_', article_title)
        article_file_name = "./generated_articles/" + article_title + "_modified" + ".txt"

        # save article to file system
        print(article['url'])
        # Split the file into sentences
        nltk.download('punkt')
        sentences = nltk.sent_tokenize(article_text)
        new_sentences = replace_random_sentence(sentences)

        # write new text to file
        with open(article_file_name, "w", encoding="utf-8") as outfile:
            for sentence in new_sentences:
                outfile.write(sentence + "\n")